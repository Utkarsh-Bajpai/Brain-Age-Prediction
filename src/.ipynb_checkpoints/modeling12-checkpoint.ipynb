{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.decomposition import KernelPCA\n",
    "from sklearn.linear_model import Ridge, ElasticNet, BayesianRidge\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update your data path\n",
    "DATA_PATH = \"/home/aunagar/Personal/Study/Sem1/Advanced ML/projects/task1/Task1/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data\n",
    "train_X = pd.read_csv(DATA_PATH + \"X_train.csv\")\n",
    "train_Y = pd.read_csv(DATA_PATH + \"y_train.csv\")\n",
    "test_X = pd.read_csv(DATA_PATH + \"X_test.csv\")\n",
    "sample_submission = pd.read_csv(DATA_PATH + \"sample.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data\n",
    "train_ids = train_X.iloc[:, 0]\n",
    "train_features = train_X.iloc[:, 1:]\n",
    "test_ids = test_X.iloc[:, 0]\n",
    "test_features = test_X.iloc[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## missing value imputation (median) ########\n",
    "# train\n",
    "train_features = train_features.fillna(train_features.mean())\n",
    "# test\n",
    "test_features = test_features.fillna(train_features.mean())\n",
    "\n",
    "\n",
    "####### limiting feature using variance threshold (i.e. remove features with 0 variance) ######\n",
    "train_features_mean, train_features_std = train_features.mean(), train_features.std()\n",
    "\n",
    "train_features = train_features.iloc[:, np.where(train_features_std > 0.0)[0]]\n",
    "test_features = test_features.iloc[:, np.where(train_features_std > 0.0)[0]]\n",
    "\n",
    "# ############## Outlier removal ###############\n",
    "# train_features_mean, train_features_std = train_features.mean(), train_features.std()\n",
    "# # train\n",
    "# train_features[train_features > train_features_mean + 2.5*train_features_std] = np.nan\n",
    "# train_features[train_features < train_features_mean -2.5*train_features_std] = np.nan\n",
    "# train_features = train_features.fillna(train_features.mean())\n",
    "\n",
    "# # test\n",
    "# test_features[test_features > train_features_mean + 2.5*train_features_std] = np.nan\n",
    "# test_features[test_features < train_features_mean - 2.5*train_features_std] = np.nan\n",
    "# test_features = test_features.fillna(train_features.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Normalization (median) #####\n",
    "# train\n",
    "train_mean, train_std = train_features.median(), train_features.std()\n",
    "train_features = (train_features - train_mean)/train_std\n",
    "# test \n",
    "test_features = (test_features - train_mean)/train_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Correlated feature removal #########\n",
    "# Create correlation matrix\n",
    "corr_matrix = train_features.corr().abs()\n",
    "# Select upper triangle of correlation matrix\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "# Find index of feature columns with correlation greater than 0.7\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > 0.7)]\n",
    "\n",
    "# train\n",
    "train_features = train_features.drop(columns = to_drop)\n",
    "# test\n",
    "test_features = test_features.drop(columns = to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4664924835961111\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ElasticNet(alpha=0.5, copy_X=True, fit_intercept=True, l1_ratio=0.5,\n",
       "           max_iter=1000, normalize=False, positive=False, precompute=False,\n",
       "           random_state=None, selection='cyclic', tol=0.0001, warm_start=False)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###### linear model\n",
    "lr = ElasticNet(alpha = 0.5, l1_ratio=0.5)\n",
    "validation_score = cross_val_score(lr, train_features, train_Y.iloc[:, 1:], cv = 5, scoring = 'r2')\n",
    "print(validation_score.mean())\n",
    "\n",
    "# train model on whole train data\n",
    "lr.fit(X = train_features, y = train_Y.iloc[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding to which features to remove\n",
    "non_zero_weights = np.where(lr.coef_ != 0.)[0]\n",
    "\n",
    "# removing these features from training data\n",
    "train_features = train_features.iloc[:, non_zero_weights]\n",
    "test_features = test_features.iloc[:, non_zero_weights]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Checking if log transformation helps\n",
    "original_corr_Y = train_features.corrwith(train_Y['y'], axis = 0)\n",
    "\n",
    "log_normalized_features = np.log(train_features + 100)\n",
    "log_normalized_features.iloc[:,0:-1] = log_normalized_features.iloc[:,0:-1].apply(lambda x: (x-x.mean())/ x.std(), axis=0)\n",
    "\n",
    "log_corr_Y = np.log((train_features+100)).corrwith(train_Y['y'], axis = 0)\n",
    "\n",
    "columns_to_transform = np.where(original_corr_Y.values < log_corr_Y.values)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features.iloc[:, columns_to_transform] = train_features.iloc[:, columns_to_transform].apply(\n",
    "                                    lambda x:(np.log(x+100) - np.log(x+100).mean())/(np.log(x+100).std()), axis = 0)\n",
    "test_features.iloc[:, columns_to_transform] = test_features.iloc[:, columns_to_transform].apply(\n",
    "                                    lambda x:(np.log(x+100) - np.log(x+100).mean())/(np.log(x+100).std()), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1          44.7485           1.6531           51.85s\n",
      "         2          40.3668           2.0942           47.12s\n",
      "         3          37.8850           1.8957           45.35s\n",
      "         4          35.6191           1.5014           44.36s\n",
      "         5          32.4507           1.5708           44.69s\n",
      "         6          31.2644           0.9937           44.65s\n",
      "         7          28.7901           1.4006           44.04s\n",
      "         8          26.5359           1.3654           43.39s\n",
      "         9          24.5544           1.2635           42.96s\n",
      "        10          24.0932           0.7475           42.91s\n",
      "        20          13.0852           0.3240           42.87s\n",
      "        30           7.6704           0.0618           41.39s\n",
      "        40           5.1496           0.0714           41.44s\n",
      "        50           3.2969           0.0133           40.56s\n",
      "        60           2.3886           0.0320           39.90s\n",
      "        70           1.6758           0.0090           39.19s\n",
      "        80           1.3468           0.0027           38.58s\n",
      "        90           1.0460           0.0094           37.89s\n",
      "       100           0.8831           0.0038           37.23s\n",
      "       200           0.1820           0.0005           31.33s\n",
      "       300           0.0543           0.0014           26.62s\n",
      "       400           0.0258          -0.0002           22.30s\n",
      "       500           0.0114           0.0006           18.20s\n",
      "       600           0.0087           0.0000           14.29s\n",
      "       700           0.0066           0.0000           10.52s\n",
      "       800           0.0058          -0.0000            6.93s\n",
      "       900           0.0122           0.0000            3.42s\n",
      "      1000           0.0050           0.0001            0.00s\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1          45.1829           1.8807           41.20s\n",
      "         2          41.3865           1.9499           41.49s\n",
      "         3          39.8809           1.8593           41.59s\n",
      "         4          36.3496           1.4082           41.49s\n",
      "         5          33.7343           1.2490           43.07s\n",
      "         6          32.3438           1.0328           42.68s\n",
      "         7          30.1803           1.0709           42.37s\n",
      "         8          27.8370           1.1361           42.31s\n",
      "         9          25.2200           1.2366           42.29s\n",
      "        10          24.8571           1.1483           42.57s\n",
      "        20          13.9084           0.3983           42.09s\n",
      "        30           8.1184           0.1373           40.89s\n",
      "        40           4.9205           0.1518           40.15s\n",
      "        50           3.4933           0.0512           39.57s\n",
      "        60           2.4168           0.0508           38.78s\n",
      "        70           1.8089           0.0193           37.98s\n",
      "        80           1.4091           0.0039           37.36s\n",
      "        90           1.0692          -0.0005           37.10s\n",
      "       100           0.8156           0.0174           36.37s\n",
      "       200           0.1838           0.0013           30.44s\n",
      "       300           0.0636           0.0008           25.78s\n",
      "       400           0.0200          -0.0000           21.47s\n",
      "       500           0.0192           0.0003           17.53s\n",
      "       600           0.0194          -0.0000           13.80s\n",
      "       700           0.0251           0.0001           10.29s\n",
      "       800           0.0077           0.0002            6.80s\n",
      "       900           0.0084           0.0000            3.38s\n",
      "      1000           0.0085           0.0000            0.00s\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1          47.4297           2.3739           48.03s\n",
      "         2          45.0018           1.4962           44.32s\n",
      "         3          42.4165           2.0312           43.22s\n",
      "         4          37.7163           2.0378           42.46s\n",
      "         5          36.0269           1.2983           43.58s\n",
      "         6          34.2369           0.7288           42.89s\n",
      "         7          31.9146           0.9330           42.45s\n",
      "         8          28.8203           1.3664           42.19s\n",
      "         9          27.4034           0.9196           41.92s\n",
      "        10          26.1452           1.0390           41.55s\n",
      "        20          14.0895           0.3596           40.44s\n",
      "        30           8.2035           0.1686           39.84s\n",
      "        40           5.1748           0.0924           39.29s\n",
      "        50           3.3800           0.0578           38.93s\n",
      "        60           2.3276           0.0332           38.58s\n",
      "        70           1.7364           0.0086           38.16s\n",
      "        80           1.2525           0.0092           37.54s\n",
      "        90           0.9932           0.0044           37.30s\n",
      "       100           0.7578           0.0082           36.54s\n",
      "       200           0.1497           0.0026           30.20s\n",
      "       300           0.0479          -0.0001           25.38s\n",
      "       400           0.0170           0.0000           21.30s\n",
      "       500           0.0093           0.0001           17.39s\n",
      "       600           0.0097           0.0002           13.76s\n",
      "       700           0.0082           0.0003           10.27s\n",
      "       800           0.0060           0.0000            6.83s\n",
      "       900           0.0044           0.0000            3.39s\n",
      "      1000           0.0045           0.0001            0.00s\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1          45.8363           1.9173           39.47s\n",
      "         2          42.9796           2.1253           39.83s\n",
      "         3          40.7825           1.0537           40.89s\n",
      "         4          36.6261           1.4262           40.47s\n",
      "         5          34.7162           1.0539           41.02s\n",
      "         6          32.8892           0.9901           41.06s\n",
      "         7          30.8670           1.1616           40.98s\n",
      "         8          27.5515           1.3950           40.88s\n",
      "         9          26.7476           1.0742           40.73s\n",
      "        10          24.8317           1.2497           40.72s\n",
      "        20          13.6979           0.2734           41.95s\n",
      "        30           8.3079           0.1177           41.23s\n",
      "        40           5.3003           0.1407           41.19s\n",
      "        50           3.7021           0.0162           41.48s\n",
      "        60           2.5814          -0.0004           40.57s\n",
      "        70           1.9437           0.0042           39.55s\n",
      "        80           1.4444           0.0059           39.13s\n",
      "        90           1.1475           0.0021           38.47s\n",
      "       100           0.8753           0.0044           37.65s\n",
      "       200           0.1473          -0.0002           31.95s\n",
      "       300           0.0413           0.0017           26.52s\n",
      "       400           0.0175           0.0003           22.67s\n",
      "       500           0.0175          -0.0000           18.62s\n",
      "       600           0.0084           0.0001           14.55s\n",
      "       700           0.0034           0.0001           10.86s\n",
      "       800           0.0090           0.0001            7.13s\n",
      "       900           0.0076           0.0002            3.50s\n",
      "      1000           0.0051          -0.0000            0.00s\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1          46.8484           2.5415           44.73s\n",
      "         2          43.7274           1.9915           43.55s\n",
      "         3          41.7179           1.7626           41.94s\n",
      "         4          37.5901           2.2978           42.13s\n",
      "         5          35.4364           1.7771           43.34s\n",
      "         6          33.7098           1.4615           42.87s\n",
      "         7          31.1062           1.3266           42.48s\n",
      "         8          29.2738           1.2832           42.18s\n",
      "         9          27.6612           1.0656           41.96s\n",
      "        10          25.2860           0.8109           41.87s\n",
      "        20          14.6415           0.2664           41.15s\n",
      "        30           8.9272           0.1683           40.10s\n",
      "        40           5.4028           0.0946           39.67s\n",
      "        50           3.6879           0.0225           39.07s\n",
      "        60           2.6146           0.0225           38.64s\n",
      "        70           1.8845           0.0120           37.93s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        80           1.4748           0.0008           37.01s\n",
      "        90           1.1396           0.0094           36.18s\n",
      "       100           0.9644          -0.0068           35.37s\n",
      "       200           0.2037           0.0039           29.52s\n",
      "       300           0.0609           0.0032           25.22s\n",
      "       400           0.0313          -0.0000           21.27s\n",
      "       500           0.0166           0.0001           17.45s\n",
      "       600           0.0152          -0.0024           13.75s\n",
      "       700           0.0064           0.0001           10.22s\n",
      "       800           0.0055          -0.0000            6.78s\n",
      "       900           0.0016           0.0000            3.37s\n",
      "      1000           0.0106           0.0001            0.00s\n",
      "0.5323093002937545\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1          45.3140           2.4796           50.99s\n",
      "         2          42.7876           1.8523           51.23s\n",
      "         3          40.8839           1.8436           51.10s\n",
      "         4          37.6727           2.0595           52.24s\n",
      "         5          35.1669           1.6952           52.80s\n",
      "         6          33.1329           1.1824           52.72s\n",
      "         7          30.2259           1.6227           52.62s\n",
      "         8          28.9305           1.0538           52.88s\n",
      "         9          27.3911           0.8826           52.77s\n",
      "        10          25.8017           0.8558           52.44s\n",
      "        20          14.2684           0.3387           51.57s\n",
      "        30           8.6689           0.2158           50.53s\n",
      "        40           5.6716           0.1369           49.56s\n",
      "        50           3.8302           0.0453           48.98s\n",
      "        60           2.6488           0.0351           48.15s\n",
      "        70           1.9921           0.0144           47.28s\n",
      "        80           1.5776           0.0010           46.38s\n",
      "        90           1.2102          -0.0024           45.25s\n",
      "       100           1.0876          -0.0066           44.20s\n",
      "       200           0.2235          -0.0002           37.29s\n",
      "       300           0.0697          -0.0002           31.56s\n",
      "       400           0.0344           0.0009           26.28s\n",
      "       500           0.0191           0.0007           21.38s\n",
      "       600           0.0109           0.0003           16.84s\n",
      "       700           0.0057           0.0000           12.55s\n",
      "       800           0.0065          -0.0013            8.26s\n",
      "       900           0.0088           0.0001            4.08s\n",
      "      1000           0.0055           0.0000            0.00s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
       "                          learning_rate=0.05, loss='huber', max_depth=7,\n",
       "                          max_features=None, max_leaf_nodes=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=10, min_samples_split=10,\n",
       "                          min_weight_fraction_leaf=0.0, n_estimators=1000,\n",
       "                          n_iter_no_change=None, presort='auto',\n",
       "                          random_state=42, subsample=0.9, tol=0.0001,\n",
       "                          validation_fraction=0.33, verbose=1,\n",
       "                          warm_start=False)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### training more complex model on this cleared date\n",
    "gbr = GradientBoostingRegressor(loss='huber', learning_rate=0.05, n_estimators=1000, subsample=0.9,\n",
    "                                criterion='friedman_mse', min_samples_split=10, min_samples_leaf=10,\n",
    "                                min_weight_fraction_leaf=0.0, max_depth=7, min_impurity_decrease=0.0,\n",
    "                                min_impurity_split=None, init=None, random_state=42, max_features=None,\n",
    "                                alpha=0.9, verbose=1, max_leaf_nodes=None, warm_start=False, presort='auto',\n",
    "                                validation_fraction=0.33, n_iter_no_change=None, tol=0.0001)\n",
    "validation_score = cross_val_score(gbr, train_features, train_Y.iloc[:, 1], cv = 5, scoring= 'r2')\n",
    "print(validation_score.mean())\n",
    "\n",
    "gbr.fit(X= train_features, y = train_Y.iloc[:, 1].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(gbr.predict(train_features), train_Y.iloc[:, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = gbr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = best_model.predict(test_features)\n",
    "sample_submission['y'] = predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission.to_csv(\"submissions/Ajay_11th_sub.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
