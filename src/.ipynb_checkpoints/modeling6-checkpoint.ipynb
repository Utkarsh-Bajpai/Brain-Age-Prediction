{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.preprocessing import scale, PolynomialFeatures\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.decomposition import KernelPCA\n",
    "from sklearn.linear_model import Ridge, ElasticNet, BayesianRidge, Lasso\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update your data path\n",
    "DATA_PATH = \"/home/aunagar/Personal/Study/Sem1/Advanced ML/projects/task1/Task1/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data\n",
    "train_X = pd.read_csv(DATA_PATH + \"X_train.csv\")\n",
    "train_Y = pd.read_csv(DATA_PATH + \"y_train.csv\")\n",
    "test_X = pd.read_csv(DATA_PATH + \"X_test.csv\")\n",
    "sample_submission = pd.read_csv(DATA_PATH + \"sample.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data\n",
    "train_ids = train_X.iloc[:, 0]\n",
    "train_features = train_X.iloc[:, 1:]\n",
    "test_ids = test_X.iloc[:, 0]\n",
    "test_features = test_X.iloc[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## missing value imputation########\n",
    "# train\n",
    "train_features = train_features.fillna(train_features.mean())\n",
    "# test\n",
    "test_features = test_features.fillna(train_features.mean())\n",
    "\n",
    "\n",
    "####### limiting feature using variance threshold (i.e. remove features with 0 variance) ######\n",
    "train_features_mean, train_features_std = train_features.mean(), train_features.std()\n",
    "\n",
    "train_features = train_features.iloc[:, np.where(train_features_std > 0.0)[0]]\n",
    "test_features = test_features.iloc[:, np.where(train_features_std > 0.0)[0]]\n",
    "\n",
    "############## Outlier removal ###############\n",
    "train_features_mean, train_features_std = train_features.mean(), train_features.std()\n",
    "# train\n",
    "train_features[train_features > train_features_mean + 2*train_features_std] = np.nan\n",
    "train_features[train_features < train_features_mean -2*train_features_std] = np.nan\n",
    "train_features = train_features.fillna(train_features.mean())\n",
    "\n",
    "# test\n",
    "test_features[test_features > train_features_mean + 2*train_features_std] = np.nan\n",
    "test_features[test_features < train_features_mean - 2*train_features_std] = np.nan\n",
    "test_features = test_features.fillna(train_features.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Normalization #####\n",
    "# train\n",
    "train_mean, train_std = train_features.mean(), train_features.std()\n",
    "train_features = (train_features - train_mean)/train_std\n",
    "# test \n",
    "test_features = (test_features - train_mean)/train_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ###### Correlated feature removal #########\n",
    "# # Create correlation matrix\n",
    "# corr_matrix = train_features.corr().abs()\n",
    "# # Select upper triangle of correlation matrix\n",
    "# upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "# # Find index of feature columns with correlation greater than 0.7\n",
    "# to_drop = [column for column in upper.columns if any(upper[column] > 0.8)]\n",
    "\n",
    "# # train\n",
    "# train_features = train_features.drop(columns = to_drop)\n",
    "# # test\n",
    "# test_features = test_features.drop(columns = to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4853832911915085\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ElasticNet(alpha=0.5, copy_X=True, fit_intercept=True, l1_ratio=0.5,\n",
       "           max_iter=1000, normalize=False, positive=False, precompute=False,\n",
       "           random_state=None, selection='cyclic', tol=0.0001, warm_start=False)"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###### linear model\n",
    "lr = ElasticNet(alpha = 0.5, l1_ratio=0.5)\n",
    "#lr = Lasso(alpha = 0.25, max_iter = 10000)\n",
    "validation_score = cross_val_score(lr, train_features, train_Y.iloc[:, 1:], cv = 5, scoring = 'r2')\n",
    "print(validation_score.mean())\n",
    "\n",
    "# train model on whole train data\n",
    "lr.fit(X = train_features, y = train_Y.iloc[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding to which features to remove\n",
    "non_zero_weights = np.where(lr.coef_ != 0.)[0]\n",
    "\n",
    "# removing these features from training data\n",
    "train_features = train_features.iloc[:, non_zero_weights]\n",
    "test_features = test_features.iloc[:, non_zero_weights]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5745746898858496\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ElasticNet(alpha=0.30000000000000004, copy_X=True, fit_intercept=True,\n",
       "           l1_ratio=0.1, max_iter=1000, normalize=False, positive=False,\n",
       "           precompute=False, random_state=None, selection='cyclic', tol=0.0001,\n",
       "           warm_start=False)"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###### linear model\n",
    "alpha = np.arange(0.1, 2, 0.2)\n",
    "l1_ratio = np.arange(0.1, 1, 0.2)\n",
    "results = pd.DataFrame(columns=['alpha', 'l1_ratio', 'cv_score'])\n",
    "for a in alpha:\n",
    "    for l1 in l1_ratio:\n",
    "        lr = ElasticNet(alpha = a, l1_ratio=l1)\n",
    "        validation_score = cross_val_score(lr, train_features, train_Y.iloc[:, 1:], cv = 5, scoring = 'r2')\n",
    "        results = results.append({'alpha':a, 'l1_ratio':l1, 'cv_score':validation_score.mean()}, ignore_index = True)\n",
    "\n",
    "best_parameters = results.iloc[np.argmax(results.cv_score.values),:]\n",
    "print(best_parameters['cv_score'])\n",
    "\n",
    "# train model on whole train data\n",
    "lr = ElasticNet(alpha = best_parameters['alpha'], l1_ratio = best_parameters['l1_ratio'])\n",
    "lr.fit(X = train_features, y = train_Y.iloc[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5512171787243063\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVR(C=4, cache_size=200, coef0=0.0, degree=10, epsilon=0.1, gamma='scale',\n",
       "    kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training support vector regression\n",
    "svr = SVR(kernel = 'rbf', degree = 10, gamma = 'scale', C = 4)\n",
    "validation_score = cross_val_score(svr, train_features, train_Y.iloc[:, 1], cv = 5, scoring = 'r2')\n",
    "print(validation_score.mean())\n",
    "\n",
    "svr.fit(train_features, y = train_Y.iloc[:, 1].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  65 tasks      | elapsed:    2.3s\n",
      "[Parallel(n_jobs=-1)]: Done 665 tasks      | elapsed:   12.6s\n",
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:   18.2s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done 792 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=4)]: Done 1000 out of 1000 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done 144 tasks      | elapsed:    2.6s\n",
      "[Parallel(n_jobs=-1)]: Done 744 tasks      | elapsed:   12.5s\n",
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:   16.7s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done 792 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=4)]: Done 1000 out of 1000 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done 144 tasks      | elapsed:    2.6s\n",
      "[Parallel(n_jobs=-1)]: Done 744 tasks      | elapsed:   12.7s\n",
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:   17.0s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done 792 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=4)]: Done 1000 out of 1000 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done 144 tasks      | elapsed:    2.6s\n",
      "[Parallel(n_jobs=-1)]: Done 744 tasks      | elapsed:   13.1s\n",
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:   17.5s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done 792 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=4)]: Done 1000 out of 1000 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done 144 tasks      | elapsed:    2.6s\n",
      "[Parallel(n_jobs=-1)]: Done 744 tasks      | elapsed:   12.8s\n",
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:   17.0s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done 792 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=4)]: Done 1000 out of 1000 | elapsed:    0.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.48087533915608016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    0.9s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:    4.6s\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:    9.7s\n",
      "[Parallel(n_jobs=-1)]: Done 792 tasks      | elapsed:   17.2s\n",
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:   21.6s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=5,\n",
       "                      max_features='auto', max_leaf_nodes=None,\n",
       "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                      min_samples_leaf=1, min_samples_split=2,\n",
       "                      min_weight_fraction_leaf=0.0, n_estimators=1000,\n",
       "                      n_jobs=-1, oob_score=False, random_state=None,\n",
       "                      verbose=True, warm_start=False)"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### training more complex model on this cleared data\n",
    "rfr = RandomForestRegressor(n_estimators=1000, max_depth=5, verbose = True, n_jobs = -1)\n",
    "validation_score = cross_val_score(rfr, train_features, train_Y.iloc[:, 1], cv = 5, scoring= 'r2')\n",
    "print(validation_score.mean())\n",
    "\n",
    "rfr.fit(X= train_features, y = train_Y.iloc[:, 1].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, Y_train, Y_val = train_test_split(train_features, train_Y.iloc[:, 1], test_size = 0.33, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select model to diagnose\n",
    "models = lr\n",
    "models.fit(X_train, Y_train)\n",
    "training_r2_score = r2_score(models.predict(X_train), Y_train)\n",
    "validation_r2_score = r2_score(models.predict(X_val), Y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.4500804279331373, 0.18308884867486985)"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_r2_score, validation_r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rfr score:  0.8970579616387858 \n",
      "\n",
      "svr score:  0.5670794383215089 \n",
      "\n",
      "lr score:  0.3883364237292618 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "rfr_r2 = r2_score(rfr.predict(train_features), train_Y.iloc[:, 1])\n",
    "lr_r2 = r2_score(lr.predict(train_features), train_Y.iloc[:, 1])\n",
    "svr_r2 = r2_score(svr.predict(train_features), train_Y.iloc[:, 1])\n",
    "\n",
    "print('rfr score: ', rfr_r2, '\\n')\n",
    "print('svr score: ', svr_r2, '\\n')\n",
    "print('lr score: ', lr_r2, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = svr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = best_model.predict(test_features)\n",
    "sample_submission['y'] = predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission.to_csv(\"submissions/Ajay_8th_sub.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
