{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.decomposition import KernelPCA\n",
    "from sklearn.linear_model import Ridge, ElasticNet, BayesianRidge\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update your data path\n",
    "DATA_PATH = \"/home/aunagar/Personal/Study/Sem1/Advanced ML/projects/task1/Task1/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data\n",
    "train_X = pd.read_csv(DATA_PATH + \"X_train.csv\")\n",
    "train_Y = pd.read_csv(DATA_PATH + \"y_train.csv\")\n",
    "test_X = pd.read_csv(DATA_PATH + \"X_test.csv\")\n",
    "sample_submission = pd.read_csv(DATA_PATH + \"sample.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data\n",
    "train_ids = train_X.iloc[:, 0]\n",
    "train_features = train_X.iloc[:, 1:]\n",
    "test_ids = test_X.iloc[:, 0]\n",
    "test_features = test_X.iloc[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## missing value imputation (median) ########\n",
    "# train\n",
    "train_features = train_features.fillna(train_features.mean())\n",
    "# test\n",
    "test_features = test_features.fillna(train_features.mean())\n",
    "\n",
    "\n",
    "####### limiting feature using variance threshold (i.e. remove features with 0 variance) ######\n",
    "train_features_mean, train_features_std = train_features.mean(), train_features.std()\n",
    "\n",
    "train_features = train_features.iloc[:, np.where(train_features_std > 0.0)[0]]\n",
    "test_features = test_features.iloc[:, np.where(train_features_std > 0.0)[0]]\n",
    "\n",
    "############## Outlier removal ###############\n",
    "train_features_mean, train_features_std = train_features.mean(), train_features.std()\n",
    "# train\n",
    "train_features[train_features > train_features_mean + 3.*train_features_std] = np.nan\n",
    "train_features[train_features < train_features_mean -3.*train_features_std] = np.nan\n",
    "train_features = train_features.fillna(train_features.mean())\n",
    "\n",
    "# test\n",
    "test_features[test_features > train_features_mean + 3.*train_features_std] = np.nan\n",
    "test_features[test_features < train_features_mean - 3.*train_features_std] = np.nan\n",
    "test_features = test_features.fillna(train_features.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Normalization (median) #####\n",
    "# train\n",
    "train_mean, train_std = train_features.median(), train_features.std()\n",
    "train_features = (train_features - train_mean)/train_std\n",
    "# test \n",
    "test_features = (test_features - train_mean)/train_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Correlated feature removal #########\n",
    "# Create correlation matrix\n",
    "corr_matrix = train_features.corr().abs()\n",
    "# Select upper triangle of correlation matrix\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "# Find index of feature columns with correlation greater than 0.7\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > 0.7)]\n",
    "\n",
    "# train\n",
    "train_features = train_features.drop(columns = to_drop)\n",
    "# test\n",
    "test_features = test_features.drop(columns = to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.42487022151743564\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ElasticNet(alpha=0.5, copy_X=True, fit_intercept=True, l1_ratio=0.5,\n",
       "           max_iter=1000, normalize=False, positive=False, precompute=False,\n",
       "           random_state=None, selection='cyclic', tol=0.0001, warm_start=False)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###### linear model\n",
    "lr = ElasticNet(alpha = 0.5, l1_ratio=0.5)\n",
    "validation_score = cross_val_score(lr, train_features, train_Y.iloc[:, 1:], cv = 5, scoring = 'r2')\n",
    "print(validation_score.mean())\n",
    "\n",
    "# train model on whole train data\n",
    "lr.fit(X = train_features, y = train_Y.iloc[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding to which features to remove\n",
    "non_zero_weights = np.where(lr.coef_ != 0.)[0]\n",
    "\n",
    "# removing these features from training data\n",
    "train_features = train_features.iloc[:, non_zero_weights]\n",
    "test_features = test_features.iloc[:, non_zero_weights]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Checking if log transformation helps\n",
    "original_corr_Y = train_features.corrwith(train_Y['y'], axis = 0)\n",
    "\n",
    "log_normalized_features = np.log(train_features + 100)\n",
    "log_normalized_features.iloc[:,0:-1] = log_normalized_features.iloc[:,0:-1].apply(lambda x: (x-x.mean())/ x.std(), axis=0)\n",
    "\n",
    "log_corr_Y = np.log((train_features+100)).corrwith(train_Y['y'], axis = 0)\n",
    "\n",
    "columns_to_transform = np.where(original_corr_Y.values < log_corr_Y.values)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features.iloc[:, columns_to_transform] = train_features.iloc[:, columns_to_transform].apply(\n",
    "                                    lambda x:(np.log(x+100) - np.log(x+100).mean())/(np.log(x+100).std()), axis = 0)\n",
    "test_features.iloc[:, columns_to_transform] = test_features.iloc[:, columns_to_transform].apply(\n",
    "                                    lambda x:(np.log(x+100) - np.log(x+100).mean())/(np.log(x+100).std()), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1          44.8414           1.5404           46.81s\n",
      "         2          40.6868           2.1040           45.55s\n",
      "         3          38.2829           1.5243           45.38s\n",
      "         4          36.1962           1.2488           45.53s\n",
      "         5          32.8489           1.3418           48.95s\n",
      "         6          31.8524           1.2189           50.67s\n",
      "         7          29.4075           1.3389           50.76s\n",
      "         8          26.9827           1.4755           52.59s\n",
      "         9          24.9205           1.4216           52.55s\n",
      "        10          24.4293           0.9961           52.27s\n",
      "        20          13.4446           0.3003           46.89s\n",
      "        30           8.0492           0.1486           47.57s\n",
      "        40           5.2728           0.0385           46.03s\n",
      "        50           3.4944           0.0476           45.46s\n",
      "        60           2.4517           0.0223           43.78s\n",
      "        70           1.7392           0.0399           42.46s\n",
      "        80           1.4011           0.0172           41.55s\n",
      "        90           1.0860          -0.0038           40.93s\n",
      "       100           0.8643          -0.0007           40.17s\n",
      "       200           0.1987           0.0055           31.65s\n",
      "       300           0.0586          -0.0001           26.44s\n",
      "       400           0.0301           0.0000           21.80s\n",
      "       500           0.0326           0.0026           17.63s\n",
      "       600           0.0127           0.0003           13.90s\n",
      "       700           0.0073           0.0001           10.25s\n",
      "       800           0.0075           0.0001            6.75s\n",
      "       900           0.0088          -0.0000            3.34s\n",
      "      1000           0.0091           0.0000            0.00s\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1          45.3139           1.7502           39.41s\n",
      "         2          41.6474           1.4913           39.17s\n",
      "         3          40.2224           1.0985           38.71s\n",
      "         4          36.7678           1.8108           39.47s\n",
      "         5          34.1600           1.4858           41.46s\n",
      "         6          32.6192           1.1707           41.24s\n",
      "         7          30.5366           1.5007           40.80s\n",
      "         8          28.1896           1.3680           40.62s\n",
      "         9          25.7554           1.3727           40.30s\n",
      "        10          25.3503           1.1780           40.06s\n",
      "        20          14.7350           0.3652           39.06s\n",
      "        30           8.7373           0.2463           38.57s\n",
      "        40           5.4641           0.1369           38.25s\n",
      "        50           3.8771           0.0384           37.61s\n",
      "        60           2.6760          -0.0029           36.98s\n",
      "        70           1.9613          -0.0055           36.22s\n",
      "        80           1.5337           0.0005           35.55s\n",
      "        90           1.1969           0.0097           34.75s\n",
      "       100           0.9204           0.0141           34.10s\n",
      "       200           0.2067          -0.0001           28.46s\n",
      "       300           0.0613           0.0001           24.25s\n",
      "       400           0.0229           0.0014           20.53s\n",
      "       500           0.0115          -0.0000           16.90s\n",
      "       600           0.0081           0.0000           13.33s\n",
      "       700           0.0133           0.0001            9.87s\n",
      "       800           0.0064           0.0002            6.51s\n",
      "       900           0.0053          -0.0000            3.25s\n",
      "      1000           0.0071          -0.0000            0.00s\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1          47.3832           2.2683           39.55s\n",
      "         2          44.9453           1.5291           39.08s\n",
      "         3          42.5331           1.7170           39.05s\n",
      "         4          37.8796           1.9550           38.68s\n",
      "         5          36.0140           1.3437           40.70s\n",
      "         6          34.2835           1.1156           40.50s\n",
      "         7          32.0194           1.5338           40.33s\n",
      "         8          28.8445           1.4503           40.10s\n",
      "         9          27.3455           1.0263           40.03s\n",
      "        10          26.1959           1.2068           39.85s\n",
      "        20          14.2334           0.3500           39.05s\n",
      "        30           8.5366           0.0939           38.51s\n",
      "        40           5.3695           0.1271           38.06s\n",
      "        50           3.5138           0.0257           37.74s\n",
      "        60           2.4526           0.0397           37.12s\n",
      "        70           1.8224           0.0107           36.57s\n",
      "        80           1.3318           0.0088           35.91s\n",
      "        90           1.0790           0.0018           35.21s\n",
      "       100           0.8461           0.0025           34.63s\n",
      "       200           0.1436          -0.0009           29.25s\n",
      "       300           0.0460          -0.0030           24.89s\n",
      "       400           0.0205           0.0007           20.98s\n",
      "       500           0.0131           0.0002           17.12s\n",
      "       600           0.0080           0.0005           13.52s\n",
      "       700           0.0046           0.0001           10.02s\n",
      "       800           0.0036          -0.0000            6.62s\n",
      "       900           0.0035           0.0000            3.29s\n",
      "      1000           0.0033           0.0002            0.00s\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1          45.7908           1.5565           41.58s\n",
      "         2          43.0148           1.8791           40.62s\n",
      "         3          40.6689           0.8949           40.21s\n",
      "         4          36.6365           1.3702           39.43s\n",
      "         5          34.5544           1.5021           41.47s\n",
      "         6          32.7775           0.8679           40.98s\n",
      "         7          30.7669           1.1123           40.65s\n",
      "         8          27.6974           1.1307           40.37s\n",
      "         9          26.8049           1.0199           40.29s\n",
      "        10          24.9046           0.8437           40.04s\n",
      "        20          13.8551           0.3671           39.35s\n",
      "        30           8.3548           0.1488           38.93s\n",
      "        40           5.1873           0.1113           38.26s\n",
      "        50           3.5548           0.0390           37.54s\n",
      "        60           2.5250           0.0084           36.98s\n",
      "        70           1.8675          -0.0024           36.43s\n",
      "        80           1.3526           0.0158           35.87s\n",
      "        90           1.0225           0.0078           35.40s\n",
      "       100           0.8431           0.0060           34.50s\n",
      "       200           0.1510           0.0050           28.89s\n",
      "       300           0.0421           0.0006           24.52s\n",
      "       400           0.0225           0.0004           20.45s\n",
      "       500           0.0129          -0.0000           16.69s\n",
      "       600           0.0088          -0.0000           13.16s\n",
      "       700           0.0037           0.0001            9.76s\n",
      "       800           0.0079           0.0000            6.45s\n",
      "       900           0.0045           0.0000            3.21s\n",
      "      1000           0.0034           0.0001            0.00s\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1          46.8675           2.0434           39.58s\n",
      "         2          44.0289           1.9839           39.36s\n",
      "         3          41.7232           1.6829           39.61s\n",
      "         4          37.8231           2.3059           39.32s\n",
      "         5          35.6697           1.7156           40.25s\n",
      "         6          33.9884           1.2347           39.95s\n",
      "         7          31.5545           1.3731           39.68s\n",
      "         8          29.6191           1.2489           39.60s\n",
      "         9          28.0782           1.1704           39.50s\n",
      "        10          25.6253           1.0733           39.41s\n",
      "        20          14.8743           0.4931           39.23s\n",
      "        30           8.9464           0.1213           38.89s\n",
      "        40           5.4517           0.0493           38.36s\n",
      "        50           3.6133           0.0541           37.71s\n",
      "        60           2.6207           0.0433           37.03s\n",
      "        70           1.9051           0.0083           36.38s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        80           1.3821           0.0093           35.84s\n",
      "        90           1.0589           0.0185           35.11s\n",
      "       100           0.8654          -0.0002           34.48s\n",
      "       200           0.1689           0.0006           28.72s\n",
      "       300           0.0490          -0.0002           24.43s\n",
      "       400           0.0229           0.0008           20.55s\n",
      "       500           0.0148           0.0000           16.82s\n",
      "       600           0.0086           0.0000           13.27s\n",
      "       700           0.0060           0.0002            9.88s\n",
      "       800           0.0068          -0.0000            6.54s\n",
      "       900           0.0060           0.0001            3.24s\n",
      "      1000           0.0073          -0.0000            0.00s\n",
      "0.5210446401337014\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1          45.3458           2.3606           51.41s\n",
      "         2          42.8138           1.7092           51.16s\n",
      "         3          40.9087           1.6595           50.66s\n",
      "         4          37.6895           1.5622           51.46s\n",
      "         5          35.3084           1.7702           51.48s\n",
      "         6          33.3153           1.3548           51.00s\n",
      "         7          30.3895           1.8072           50.85s\n",
      "         8          29.0916           0.9640           51.08s\n",
      "         9          27.6596           0.9831           51.73s\n",
      "        10          26.0551           0.8860           51.41s\n",
      "        20          14.6273           0.3928           50.46s\n",
      "        30           8.7823           0.2305           49.67s\n",
      "        40           5.7604           0.0941           49.00s\n",
      "        50           3.9823           0.0071           48.16s\n",
      "        60           2.6662           0.0119           47.28s\n",
      "        70           2.0485           0.0127           46.51s\n",
      "        80           1.6385           0.0077           45.66s\n",
      "        90           1.2359           0.0171           44.75s\n",
      "       100           1.0629          -0.0014           43.83s\n",
      "       200           0.2114           0.0047           36.65s\n",
      "       300           0.0709           0.0052           30.93s\n",
      "       400           0.0369           0.0009           25.91s\n",
      "       500           0.0174           0.0002           21.30s\n",
      "       600           0.0179           0.0002           16.80s\n",
      "       700           0.0084           0.0021           12.48s\n",
      "       800           0.0072          -0.0001            8.25s\n",
      "       900           0.0063           0.0001            4.09s\n",
      "      1000           0.0057           0.0000            0.00s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
       "                          learning_rate=0.05, loss='huber', max_depth=7,\n",
       "                          max_features=None, max_leaf_nodes=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=10, min_samples_split=10,\n",
       "                          min_weight_fraction_leaf=0.0, n_estimators=1000,\n",
       "                          n_iter_no_change=None, presort='auto',\n",
       "                          random_state=42, subsample=0.9, tol=0.0001,\n",
       "                          validation_fraction=0.33, verbose=1,\n",
       "                          warm_start=False)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### training more complex model on this cleared date\n",
    "gbr = GradientBoostingRegressor(loss='huber', learning_rate=0.05, n_estimators=1000, subsample=0.9,\n",
    "                                criterion='friedman_mse', min_samples_split=10, min_samples_leaf=10,\n",
    "                                min_weight_fraction_leaf=0.0, max_depth=7, min_impurity_decrease=0.0,\n",
    "                                min_impurity_split=None, init=None, random_state=42, max_features=None,\n",
    "                                alpha=0.9, verbose=1, max_leaf_nodes=None, warm_start=False, presort='auto',\n",
    "                                validation_fraction=0.33, n_iter_no_change=None, tol=0.0001)\n",
    "validation_score = cross_val_score(gbr, train_features, train_Y.iloc[:, 1], cv = 5, scoring= 'r2')\n",
    "print(validation_score.mean())\n",
    "\n",
    "gbr.fit(X= train_features, y = train_Y.iloc[:, 1].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(gbr.predict(train_features), train_Y.iloc[:, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = gbr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = best_model.predict(test_features)\n",
    "sample_submission['y'] = predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission.to_csv(\"submissions/Ajay_11th_sub.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
