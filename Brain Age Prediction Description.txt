Steps:
1.impute median
2. remove outliers via std-deviation from mean
3. impute outliers with mean
4. do standard scaling
5. remove correlated features/features with no variance
6. use lasso to identify useless features
7. train an elastic net on the remaining features with your own cross-validation function that shuffles data



I do a similar thing to what you did:
1. impute
2. remove features with too low/too high varianced
3. use robust scaler to remove and then imoute outliers
4. scale with robust scaler
5. use elasticnet to find relevant featured
6. train and print some models, such as regression, the tree thing and a bayesian regression



I achieved my score through the gridsearch, the filename shows what I did:
uv1e12: dropped all columns with a std > 1e12
lv1e-8: dropped all columns with a std < 1e-8
uc0.8 dropped columns with correlation > 0.8
dev1.6: imputed values that are > 1.6 IQR from the median away with the mean (performed better than the median)
simple/iter: used SImpleImputer/IterativeImputer.
The exact steps are shown in the main function of attempt3.py




(1) Filled missing values with mean
(2) Removed outliers from all the columns (2.5 std criteria)
(3) Scaled every features to 0 mean and 1 std
(4) Removed variables which are correlated with any other variable (threshold 0.8)
(5) Fitted an ElasticNet model (with alpha = 0.5, l1_ratio = 0.5)
(6) Removed variables which had 0 coefficients in this model
(7) Fitted a random forest regressor on remaining ones (estimators = 1000, max depth = 15) 


I've done outlier detection using Modified Z Score and Inter Quartile Range. Then removed highly correlated features and features with low standard deviation